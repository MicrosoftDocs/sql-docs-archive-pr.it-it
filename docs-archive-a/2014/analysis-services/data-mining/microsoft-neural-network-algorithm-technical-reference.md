---
title: Riferimento tecnico per l'algoritmo Microsoft Neural Network | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 08/04/2020
ms.locfileid: "87623604"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="87a5d-102">Microsoft Neural Network Algorithm Technical Reference</span><span class="sxs-lookup"><span data-stu-id="87a5d-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="87a5d-103">L'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network usa una rete *perceptron multistrato* , chiamata anche *rete Delta Rule con retropropagazione*, costituita da un massimo di tre livelli neurali o *perceptron*.</span><span class="sxs-lookup"><span data-stu-id="87a5d-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="87a5d-104">Tali livelli rappresentano rispettivamente un livello di input, un livello nascosto facoltativo e un livello di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="87a5d-105">Una descrizione dettagliata delle reti neurali perceptron multistrato esula dagli argomenti trattati nella presente documentazione.</span><span class="sxs-lookup"><span data-stu-id="87a5d-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="87a5d-106">In questo argomento viene illustrata l'implementazione di base dell'algoritmo, inclusi il metodo utilizzato per normalizzare i valori di input e output e i metodi relativi alla caratteristica di selezione degli attributi utilizzati per ridurre la cardinalità degli attributi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="87a5d-107">Vengono inoltre descritti i parametri e altre impostazioni che è possibile utilizzare per personalizzare il comportamento dell'algoritmo e vengono forniti collegamenti a informazioni aggiuntive relative all'esecuzione di query sul modello.</span><span class="sxs-lookup"><span data-stu-id="87a5d-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="87a5d-108">Implementazione dell'algoritmo Microsoft Neural Network</span><span class="sxs-lookup"><span data-stu-id="87a5d-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="87a5d-109">In una rete neurale perceptron multistrato ogni neurone riceve uno o più input e produce uno o più output identici.</span><span class="sxs-lookup"><span data-stu-id="87a5d-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="87a5d-110">Ogni output è una funzione semplice non lineare della somma degli input ricevuti dal neurone.</span><span class="sxs-lookup"><span data-stu-id="87a5d-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="87a5d-111">Gli input passano dai nodi del livello di input ai nodi del livello nascosto, quindi ai nodi del livello di output. Non esistono connessioni tra i neuroni all'interno di un livello.</span><span class="sxs-lookup"><span data-stu-id="87a5d-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="87a5d-112">Se non è disponibile un livello nascosto, come accade nei modelli di regressione logistica, gli input passano direttamente dai nodi del livello di input ai nodi del livello di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="87a5d-113">Una rete neurale creata con l'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network include tre tipi di neuroni:</span><span class="sxs-lookup"><span data-stu-id="87a5d-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="87a5d-114">I neuroni di input forniscono i valori degli attributi per il modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="87a5d-115">Per gli attributi di input discreti, un neurone di input rappresenta generalmente un singolo stato derivato da questo tipo di attributo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="87a5d-116">Se i dati di training contengono valori Null per l'attributo, sono inclusi anche i valori mancanti.</span><span class="sxs-lookup"><span data-stu-id="87a5d-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="87a5d-117">Se nei dati di training sono inclusi valori Null, un attributo di input discreto che include più di due stati genera un neurone di input per ogni stato e un neurone di input per uno stato mancante.</span><span class="sxs-lookup"><span data-stu-id="87a5d-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="87a5d-118">Un attributo di input continuo genera due neuroni di input: un neurone per uno stato mancante e uno per il valore dell'attributo continuo stesso.</span><span class="sxs-lookup"><span data-stu-id="87a5d-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="87a5d-119">I neuroni di input inviano gli input a uno o più neuroni nascosti.</span><span class="sxs-lookup"><span data-stu-id="87a5d-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="87a5d-120">I neuroni nascosti ricevono gli input dai neuroni di input e inviano gli output ai neuroni di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="87a5d-121">I neuroni di output rappresentano i valori degli attributi stimabili per il modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="87a5d-122">Per gli attributi di input discreti, un neurone di output rappresenta generalmente un singolo stato stimato per un attributo stimabile, inclusi i valori mancanti.</span><span class="sxs-lookup"><span data-stu-id="87a5d-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="87a5d-123">Ad esempio, un attributo stimabile binario genera un nodo di output che descrive uno stato mancante o esistente, indicando se è disponibile un valore per tale attributo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="87a5d-124">Una colonna booleana utilizzata come attributo stimabile genera tre neuroni di output: un neurone per i valori True, uno per i valori False e un altro per uno stato mancante o esistente.</span><span class="sxs-lookup"><span data-stu-id="87a5d-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="87a5d-125">Un attributo stimabile discreto che include più di due stati genera un neurone di output per ogni stato e un neurone di output per uno stato mancante o esistente.</span><span class="sxs-lookup"><span data-stu-id="87a5d-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="87a5d-126">Le colonne stimabili continue generano due neuroni di output: un neurone per uno stato mancante o esistente e uno per il valore della colonna continua stessa.</span><span class="sxs-lookup"><span data-stu-id="87a5d-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="87a5d-127">Se vengono generati più di 500 neuroni di output esaminando il set di colonne stimabili, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] genera un nuova rete nel modello di data mining per rappresentare i neuroni di output aggiuntivi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="87a5d-128">Un neurone riceve l'input dagli altri neuroni o da altri dati, a seconda del livello della rete in cui si trova.</span><span class="sxs-lookup"><span data-stu-id="87a5d-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="87a5d-129">Un neurone di input riceve gli input dai dati originali.</span><span class="sxs-lookup"><span data-stu-id="87a5d-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="87a5d-130">I neuroni di output e i neuroni nascosti ricevono gli input dall'output degli altri neuroni della rete neurale.</span><span class="sxs-lookup"><span data-stu-id="87a5d-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="87a5d-131">Gli input stabiliscono le relazioni tra i neuroni e le relazioni vengono utilizzate come percorso di analisi per un set di case specifico.</span><span class="sxs-lookup"><span data-stu-id="87a5d-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="87a5d-132">A ogni input viene assegnato un valore, denominato *peso*, che ne rappresenta la pertinenza o la priorità per il neurone nascosto o per il neurone di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="87a5d-133">Maggiore è il peso assegnato a un input, più rilevante, o prioritario, è il rispettivo valore.</span><span class="sxs-lookup"><span data-stu-id="87a5d-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="87a5d-134">I pesi possono essere negativi, ovvero l'input può inibire un neurone specifico anziché attivarlo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="87a5d-135">Il valore di ogni input viene moltiplicato per il peso in modo da evidenziare la rispettiva priorità per un neurone specifico.</span><span class="sxs-lookup"><span data-stu-id="87a5d-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="87a5d-136">Nel caso di pesi negativi, moltiplicando il valore per il peso si riduce la priorità dell'input.</span><span class="sxs-lookup"><span data-stu-id="87a5d-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="87a5d-137">A ogni neurone viene assegnata una funzione semplice non lineare, denominata *funzione di attivazione*, che ne rappresenta la pertinenza o la priorità per il livello corrispondente di una rete neurale.</span><span class="sxs-lookup"><span data-stu-id="87a5d-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="87a5d-138">Per la funzione di attivazione, i neuroni nascosti usano una funzione di *tangente iperbolica* (tanh), mentre i neuroni di output utilizzano una funzione *sigmoidale* .</span><span class="sxs-lookup"><span data-stu-id="87a5d-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="87a5d-139">Entrambe le funzioni sono funzioni non lineari continue che consentono alla rete neurale di modellare relazioni non lineari tra i neuroni di input e di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="87a5d-140">Reti neurali di training</span><span class="sxs-lookup"><span data-stu-id="87a5d-140">Training Neural Networks</span></span>  
 <span data-ttu-id="87a5d-141">Il training di un modello di data mining che utilizza l'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network implica varie operazioni.</span><span class="sxs-lookup"><span data-stu-id="87a5d-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="87a5d-142">Tali operazioni vengono notevolmente influenzate dai valori specificati per i parametri disponibili per l'algoritmo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="87a5d-143">L'algoritmo inizialmente valuta ed estrae i dati di training dall'origine dei dati.</span><span class="sxs-lookup"><span data-stu-id="87a5d-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="87a5d-144">Una percentuale dei dati di training, denominati *dati di controllo*, viene riservata per la valutazione dell'accuratezza della rete.</span><span class="sxs-lookup"><span data-stu-id="87a5d-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="87a5d-145">Durante il processo di training, la rete viene valutata immediatamente dopo ogni iterazione sui dati di training.</span><span class="sxs-lookup"><span data-stu-id="87a5d-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="87a5d-146">Quando l'accuratezza non aumenta più, il processo di training viene arrestato.</span><span class="sxs-lookup"><span data-stu-id="87a5d-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="87a5d-147">I valori dei parametri *SAMPLE_SIZE* e *HOLDOUT_PERCENTAGE* vengono usati per determinare il numero di case di campionamento da estrarre dai dati di training e il numero di case da riservare per i dati di controllo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="87a5d-148">Il valore del parametro *HOLDOUT_SEED* viene usato per la determinazione casuale dei singoli case da riservare per i dati di controllo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="87a5d-149">Questi parametri dell'algoritmo sono diversi dalle proprietà HOLDOUT_SIZE e HOLDOUT_SEED, che vengono applicate a una struttura di data mining per definire un set di dati di testing.</span><span class="sxs-lookup"><span data-stu-id="87a5d-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="87a5d-150">Successivamente l'algoritmo determina il numero e la complessità delle reti supportate dal modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="87a5d-151">Se il modello di data mining contiene uno o più attributi utilizzati solo per la stima, l'algoritmo crea una singola rete che rappresenta tutti gli attributi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="87a5d-152">Se il modello di data mining contiene uno o più attributi utilizzati sia per l'input che per la stima, il provider dell'algoritmo costruisce una rete per ogni attributo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="87a5d-153">Per gli attributi di input e stimabili con valori discreti, ogni neurone di input o di output rappresenta rispettivamente un singolo stato.</span><span class="sxs-lookup"><span data-stu-id="87a5d-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="87a5d-154">Per gli attributi di input e stimabili con valori continui, ogni neurone di input o di output rappresenta rispettivamente l'intervallo e la distribuzione dei valori dell'attributo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="87a5d-155">Il numero massimo di stati supportato in ognuno dei due casi dipende dal valore del parametro *MAXIMUM_STATES* dell'algoritmo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="87a5d-156">Se il numero di stati per un attributo specifico supera il valore del parametro *MAXIMUM_STATES* dell'algoritmo, vengono scelti gli stati più frequenti o rilevanti per tale attributo, fino al numero massimo di stati consentiti, mentre gli stati rimanenti vengono raggruppati come valori mancanti da usare a scopo di analisi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="87a5d-157">L'algoritmo usa quindi il valore del parametro *HIDDEN_NODE_RATIO* durante la determinazione del numero iniziale di neuroni per creare il livello nascosto.</span><span class="sxs-lookup"><span data-stu-id="87a5d-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="87a5d-158">È possibile impostare il parametro *HIDDEN_NODE_RATIO* su 0 per impedire la creazione di un livello nascosto nelle reti generate dall'algoritmo per il modello di data mining, in modo che la rete neurale venga considerata come una regressione logistica.</span><span class="sxs-lookup"><span data-stu-id="87a5d-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="87a5d-159">Il provider dell'algoritmo valuta contemporaneamente il peso di tutti gli input della rete in modo iterativo, considerando il set di dati di training riservato in precedenza e confrontando il valore noto effettivo per ogni case nei dati di controllo con la stima della rete, mediante un processo noto come *apprendimento in batch*.</span><span class="sxs-lookup"><span data-stu-id="87a5d-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="87a5d-160">Dopo che l'algoritmo ha valutato l'intero set di dati di training, esamina il valore stimato ed effettivo per ogni neurone.</span><span class="sxs-lookup"><span data-stu-id="87a5d-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="87a5d-161">L'algoritmo calcola l'eventuale grado di errore e modifica i pesi associati agli input di tale neurone, procedendo a ritroso dai neuroni di output ai neuroni di input in un processo noto come *retropropagazione*.</span><span class="sxs-lookup"><span data-stu-id="87a5d-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="87a5d-162">Successivamente, l'algoritmo ripete il processo sull'intero set di dati di training.</span><span class="sxs-lookup"><span data-stu-id="87a5d-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="87a5d-163">Poiché l'algoritmo può supportare molti pesi e neuroni di output, viene utilizzato l'algoritmo basato su gradienti coniugati per gestire il processo di training al fine di assegnare e valutare i pesi per gli input.</span><span class="sxs-lookup"><span data-stu-id="87a5d-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="87a5d-164">Una descrizione dettagliata dell'algoritmo basato su gradienti coniugati esula dagli argomenti trattati nella presente documentazione.</span><span class="sxs-lookup"><span data-stu-id="87a5d-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="87a5d-165">Selezione caratteristiche</span><span class="sxs-lookup"><span data-stu-id="87a5d-165">Feature Selection</span></span>  
 <span data-ttu-id="87a5d-166">Se il numero degli attributi di input è maggiore del valore del parametro *MAXIMUM_INPUT_ATTRIBUTES* , oppure se il numero di attributi stimabili è maggiore del valore del parametro *MAXIMUM_OUTPUT_ATTRIBUTES* , viene usato un algoritmo di selezione delle caratteristiche per ridurre la complessità delle reti incluse nel modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="87a5d-167">La caratteristica di selezione degli attributi riduce il numero degli attributi di input o stimabili, in quanto vengono scelti quelli statisticamente più rilevanti per il modello.</span><span class="sxs-lookup"><span data-stu-id="87a5d-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="87a5d-168">Tutti gli algoritmi di data mining [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] utilizzano automaticamente la caratteristica di selezione degli attributi per migliorare l'analisi e ridurre il carico di elaborazione.</span><span class="sxs-lookup"><span data-stu-id="87a5d-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="87a5d-169">Il metodo utilizzato per la caratteristica di selezione degli attributi nei modelli di reti neurali dipende dal tipo di dati dell'attributo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="87a5d-170">Nella tabella seguente sono mostrati per riferimento i metodi relativi alla caratteristica di selezione degli attributi utilizzati per i modelli di reti neurali e quelli utilizzati per l'algoritmo Logistic Regression, che si basa sull'algoritmo Neural Network.</span><span class="sxs-lookup"><span data-stu-id="87a5d-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="87a5d-171">Algoritmo</span><span class="sxs-lookup"><span data-stu-id="87a5d-171">Algorithm</span></span>|<span data-ttu-id="87a5d-172">Metodo di analisi</span><span class="sxs-lookup"><span data-stu-id="87a5d-172">Method of analysis</span></span>|<span data-ttu-id="87a5d-173">Commenti</span><span class="sxs-lookup"><span data-stu-id="87a5d-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="87a5d-174">Neural Network</span><span class="sxs-lookup"><span data-stu-id="87a5d-174">Neural Network</span></span>|<span data-ttu-id="87a5d-175">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="87a5d-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="87a5d-176">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="87a5d-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="87a5d-177">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="87a5d-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="87a5d-178">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="87a5d-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="87a5d-179">Nell'algoritmo Neural Network possono essere utilizzati sia i metodi Bayes sia quelli basati sull'entropia, purché nei dati siano contenute colonne continue.</span><span class="sxs-lookup"><span data-stu-id="87a5d-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="87a5d-180">Predefinita.</span><span class="sxs-lookup"><span data-stu-id="87a5d-180">Default.</span></span>|  
|<span data-ttu-id="87a5d-181">Logistic Regression</span><span class="sxs-lookup"><span data-stu-id="87a5d-181">Logistic Regression</span></span>|<span data-ttu-id="87a5d-182">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="87a5d-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="87a5d-183">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="87a5d-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="87a5d-184">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="87a5d-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="87a5d-185">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="87a5d-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="87a5d-186">Poiché non è possibile passare un parametro a questo algoritmo per controllare il comportamento della caratteristica di selezione degli attributi, vengono utilizzate le impostazioni predefinite.</span><span class="sxs-lookup"><span data-stu-id="87a5d-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="87a5d-187">Se pertanto tutti gli attributi sono discreti o discretizzati, l'impostazione predefinita è BDEU.</span><span class="sxs-lookup"><span data-stu-id="87a5d-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="87a5d-188">I parametri dell'algoritmo che controllano la caratteristica di selezione degli attributi per un modello di rete neurale sono MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES e MAXIMUM_STATES.</span><span class="sxs-lookup"><span data-stu-id="87a5d-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="87a5d-189">È inoltre possibile controllare il numero di livelli nascosti impostando il parametro HIDDEN_NODE_RATIO.</span><span class="sxs-lookup"><span data-stu-id="87a5d-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="87a5d-190">Metodi di valutazione</span><span class="sxs-lookup"><span data-stu-id="87a5d-190">Scoring Methods</span></span>  
 <span data-ttu-id="87a5d-191">La*valutazione* è una sorta di normalizzazione che, nel contesto del training di un modello di rete neurale, indica il processo di conversione di un valore, ad esempio un'etichetta di testo discreta, in un valore che può essere confrontato con altri tipi di input e ponderato nella rete.</span><span class="sxs-lookup"><span data-stu-id="87a5d-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="87a5d-192">Se ad esempio un attributo di input è Gender e i valori possibili sono Male e Female, mentre un altro attributo di input è Income, con un intervallo variabile di valori, i valori per ogni attributo non sono direttamente confrontabili e pertanto devono essere codificati in una scala comune in modo da consentirne il calcolo del peso.</span><span class="sxs-lookup"><span data-stu-id="87a5d-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="87a5d-193">La valutazione è il processo in base al quale tali input vengono normalizzati in valori numerici, nello specifico in un intervallo di probabilità.</span><span class="sxs-lookup"><span data-stu-id="87a5d-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="87a5d-194">Le funzioni utilizzate per la normalizzazione consentono inoltre di distribuire in modo più uniforme il valore di input in modo che valori estremi non alterino i risultati dell'analisi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="87a5d-195">Vengono inoltre codificati gli output della rete neurale.</span><span class="sxs-lookup"><span data-stu-id="87a5d-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="87a5d-196">Quando per l'output è disponibile una sola destinazione (ovvero la stima) oppure più destinazioni utilizzate esclusivamente per la stima e non per l'input, il modello crea una sola rete e la normalizzazione dei valori potrebbe sembrare non necessaria.</span><span class="sxs-lookup"><span data-stu-id="87a5d-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="87a5d-197">Se tuttavia per l'input e la stima vengono utilizzati più attributi, il modello deve creare più reti, pertanto tutti i valori devono essere normalizzati e anche gli output devono essere codificati quando escono dalla rete.</span><span class="sxs-lookup"><span data-stu-id="87a5d-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="87a5d-198">La codifica per gli input si basa sulla somma di ogni valore discreto nei case di training e sulla moltiplicazione del valore ottenuto per il rispettivo peso.</span><span class="sxs-lookup"><span data-stu-id="87a5d-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="87a5d-199">Questa operazione viene definita *somma ponderata*e viene passata alla funzione di attivazione nel livello nascosto.</span><span class="sxs-lookup"><span data-stu-id="87a5d-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="87a5d-200">Per la codifica viene utilizzato un punteggio z, come illustrato di seguito:</span><span class="sxs-lookup"><span data-stu-id="87a5d-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="87a5d-201">**Valori discreti**</span><span class="sxs-lookup"><span data-stu-id="87a5d-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="87a5d-202">μ = p: probabilità precedente di uno stato</span><span class="sxs-lookup"><span data-stu-id="87a5d-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="87a5d-203">StdDev = sqrt (p (1-p))</span><span class="sxs-lookup"><span data-stu-id="87a5d-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="87a5d-204">**Valori continui**</span><span class="sxs-lookup"><span data-stu-id="87a5d-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="87a5d-205">Valore presente = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="87a5d-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="87a5d-206">Nessun valore esistente =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="87a5d-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="87a5d-207">Dopo che i valori sono stati codificati, gli input vengono sottoposti alla somma ponderata, in cui i pesi sono rappresentati dai bordi delle reti.</span><span class="sxs-lookup"><span data-stu-id="87a5d-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="87a5d-208">Per la codifica degli output viene utilizzata la funzione sigmoidale, le cui proprietà la rendono molto utile per la stima.</span><span class="sxs-lookup"><span data-stu-id="87a5d-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="87a5d-209">In base a una di tali proprietà, indipendentemente da come vengono ridimensionati i valori originali e indipendentemente dal fatto che tali valori siano negativi o positivi, l'output di questa funzione è sempre un valore compreso tra 0 e 1, ovvero un valore appropriato per la stima delle probabilità.</span><span class="sxs-lookup"><span data-stu-id="87a5d-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="87a5d-210">Un'altra proprietà utile è la funzione sigmoidale con effetto di smussatura, in base alla quale man mano che i valori si allontanano dal punto di flesso, la probabilità del valore si sposta lentamente verso 0 o 1.</span><span class="sxs-lookup"><span data-stu-id="87a5d-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="87a5d-211">Personalizzazione dell'algoritmo Neural Network</span><span class="sxs-lookup"><span data-stu-id="87a5d-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="87a5d-212">L'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network supporta vari parametri che influiscono sul comportamento, sulle prestazioni e sull'accuratezza del modello di data mining risultante.</span><span class="sxs-lookup"><span data-stu-id="87a5d-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="87a5d-213">È inoltre possibile modificare la modalità con cui il modello elabora i dati impostando flag di modellazione nelle colonne oppure impostando flag di distribuzione per specificare come devono essere gestiti i valori all'interno della colonna.</span><span class="sxs-lookup"><span data-stu-id="87a5d-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="87a5d-214">Impostazione dei parametri dell'algoritmo</span><span class="sxs-lookup"><span data-stu-id="87a5d-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="87a5d-215">Nella tabella seguente vengono descritti i parametri che possono essere utilizzati con l'algoritmo Microsoft Neural Network.</span><span class="sxs-lookup"><span data-stu-id="87a5d-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="87a5d-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="87a5d-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="87a5d-217">Specifica il rapporto tra i neuroni nascosti e i neuroni di input e di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="87a5d-218">La formula seguente consente di determinare il numero iniziale di neuroni nel livello nascosto:</span><span class="sxs-lookup"><span data-stu-id="87a5d-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="87a5d-219">HIDDEN_NODE_RATIO \* SQRT (Totale neuroni di input \* Totale neuroni di output)</span><span class="sxs-lookup"><span data-stu-id="87a5d-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="87a5d-220">Il valore predefinito è 4,0.</span><span class="sxs-lookup"><span data-stu-id="87a5d-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="87a5d-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="87a5d-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="87a5d-222">Specifica la percentuale di case all'interno dei dati di training utilizzata per calcolare l'errore dei dati di controllo nell'ambito dei criteri di interruzione durante l'esecuzione del training del modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="87a5d-223">Il valore predefinito è 30.</span><span class="sxs-lookup"><span data-stu-id="87a5d-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="87a5d-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="87a5d-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="87a5d-225">Specifica un numero utilizzato come valore di inizializzazione per il generatore pseudocasuale quando l'algoritmo determina in modo casuale i dati di controllo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="87a5d-226">Se questo parametro è impostato su 0, l'algoritmo genera il valore di inizializzazione in base al nome del modello di data mining, per garantire che il contenuto del modello rimanga invariato durante la rielaborazione.</span><span class="sxs-lookup"><span data-stu-id="87a5d-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="87a5d-227">Il valore predefinito è 0.</span><span class="sxs-lookup"><span data-stu-id="87a5d-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="87a5d-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="87a5d-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="87a5d-229">Determina il numero massimo di attributi di input che è possibile fornire all'algoritmo prima di utilizzare la caratteristica di selezione degli attributi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="87a5d-230">Se si imposta il valore su 0, la caratteristica di selezione degli attributi viene disabilitata per gli attributi di input.</span><span class="sxs-lookup"><span data-stu-id="87a5d-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="87a5d-231">Il valore predefinito è 255.</span><span class="sxs-lookup"><span data-stu-id="87a5d-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="87a5d-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="87a5d-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="87a5d-233">Determina il numero massimo di attributi di output che è possibile fornire all'algoritmo prima di utilizzare la caratteristica di selezione degli attributi.</span><span class="sxs-lookup"><span data-stu-id="87a5d-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="87a5d-234">Se si imposta il valore su 0, la caratteristica di selezione degli attributi viene disabilitata per gli attributi di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="87a5d-235">Il valore predefinito è 255.</span><span class="sxs-lookup"><span data-stu-id="87a5d-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="87a5d-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="87a5d-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="87a5d-237">Specifica il numero massimo di stati discreti per attributo supportato dall'algoritmo.</span><span class="sxs-lookup"><span data-stu-id="87a5d-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="87a5d-238">Se il numero di stati per un attributo specifico è maggiore del numero specificato per questo parametro, l'algoritmo utilizza gli stati più frequenti per tale attributo e considera gli stati rimanenti come mancanti.</span><span class="sxs-lookup"><span data-stu-id="87a5d-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="87a5d-239">Il valore predefinito è 100.</span><span class="sxs-lookup"><span data-stu-id="87a5d-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="87a5d-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="87a5d-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="87a5d-241">Specifica il numero di case da utilizzare per eseguire il training del modello.</span><span class="sxs-lookup"><span data-stu-id="87a5d-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="87a5d-242">L'algoritmo utilizza il valore minore tra questo numero e la percentuale del numero totale di case non inclusi nei dati di controllo specificata dal parametro HOLDOUT_PERCENTAGE.</span><span class="sxs-lookup"><span data-stu-id="87a5d-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="87a5d-243">In altre parole, se il parametro HOLDOUT_PERCENTAGE è impostato su 30, l'algoritmo utilizzerà il valore di questo parametro o un valore uguale al 70% del numero totale di case, a seconda del valore minore.</span><span class="sxs-lookup"><span data-stu-id="87a5d-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="87a5d-244">Il valore predefinito è 10000.</span><span class="sxs-lookup"><span data-stu-id="87a5d-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="87a5d-245">Flag di modellazione</span><span class="sxs-lookup"><span data-stu-id="87a5d-245">Modeling Flags</span></span>  
 <span data-ttu-id="87a5d-246">Di seguito sono indicati i flag di modellazione che è possibile utilizzare con l'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network.</span><span class="sxs-lookup"><span data-stu-id="87a5d-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="87a5d-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="87a5d-247">NOT NULL</span></span>  
 <span data-ttu-id="87a5d-248">Indica che la colonna non può contenere un valore Null.</span><span class="sxs-lookup"><span data-stu-id="87a5d-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="87a5d-249">Se Analysis Services rileva un valore Null durante il training del modello, viene generato un errore.</span><span class="sxs-lookup"><span data-stu-id="87a5d-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="87a5d-250">Si applica alle colonne della struttura di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="87a5d-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="87a5d-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="87a5d-252">Indica che per il modello si deve considerare solo se un valore di attributo esiste oppure manca.</span><span class="sxs-lookup"><span data-stu-id="87a5d-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="87a5d-253">Non è importante il valore esatto.</span><span class="sxs-lookup"><span data-stu-id="87a5d-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="87a5d-254">Si applica alle colonne del modello di data mining.</span><span class="sxs-lookup"><span data-stu-id="87a5d-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="87a5d-255">Flag di distribuzione</span><span class="sxs-lookup"><span data-stu-id="87a5d-255">Distribution Flags</span></span>  
 <span data-ttu-id="87a5d-256">Di seguito sono indicati i flag di distribuzione il cui utilizzo è supportato con l'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network.</span><span class="sxs-lookup"><span data-stu-id="87a5d-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="87a5d-257">I flag sono utilizzati solo come hint per il modello. Se l'algoritmo rileva una distribuzione diversa, verrà utilizzata la distribuzione rilevata, non la distribuzione fornita nell'hint.</span><span class="sxs-lookup"><span data-stu-id="87a5d-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="87a5d-258">Normale</span><span class="sxs-lookup"><span data-stu-id="87a5d-258">Normal</span></span>  
 <span data-ttu-id="87a5d-259">Indica che valori all'interno della colonna devono essere trattati come se rappresentassero la distribuzione normale o di Gauss.</span><span class="sxs-lookup"><span data-stu-id="87a5d-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="87a5d-260">Uniforme</span><span class="sxs-lookup"><span data-stu-id="87a5d-260">Uniform</span></span>  
 <span data-ttu-id="87a5d-261">Indica che valori all'interno della colonna devono essere trattati come se fossero distribuiti uniformemente, ovvero la probabilità di qualsiasi valore è approssimativamente uguale ed è una funzione del numero complessivo di valori.</span><span class="sxs-lookup"><span data-stu-id="87a5d-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="87a5d-262">Logaritmica normale</span><span class="sxs-lookup"><span data-stu-id="87a5d-262">Log Normal</span></span>  
 <span data-ttu-id="87a5d-263">Indica che valori all'interno della colonna devono essere trattati come se fossero distribuiti in base alla curva di *logaritmo normale* , ovvero il logaritmo dei valori è distribuito in modo normale.</span><span class="sxs-lookup"><span data-stu-id="87a5d-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="87a5d-264">Requisiti</span><span class="sxs-lookup"><span data-stu-id="87a5d-264">Requirements</span></span>  
 <span data-ttu-id="87a5d-265">Un modello di rete neurale deve contenere almeno una colonna di input e una colonna di output.</span><span class="sxs-lookup"><span data-stu-id="87a5d-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="87a5d-266">Colonne di input e stimabili</span><span class="sxs-lookup"><span data-stu-id="87a5d-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="87a5d-267">L'algoritmo [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network supporta le colonne di input e le colonne stimabili specifiche riportate nella tabella seguente.</span><span class="sxs-lookup"><span data-stu-id="87a5d-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="87a5d-268">Colonna</span><span class="sxs-lookup"><span data-stu-id="87a5d-268">Column</span></span>|<span data-ttu-id="87a5d-269">Tipi di contenuto</span><span class="sxs-lookup"><span data-stu-id="87a5d-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="87a5d-270">Attributo di input</span><span class="sxs-lookup"><span data-stu-id="87a5d-270">Input attribute</span></span>|<span data-ttu-id="87a5d-271">Continuous, Cyclical, Discrete, Discretized, Key, Table e Ordered</span><span class="sxs-lookup"><span data-stu-id="87a5d-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="87a5d-272">Attributo stimabile</span><span class="sxs-lookup"><span data-stu-id="87a5d-272">Predictable attribute</span></span>|<span data-ttu-id="87a5d-273">Continuous, Cyclical, Discrete, Discretized e Ordered</span><span class="sxs-lookup"><span data-stu-id="87a5d-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="87a5d-274">Sono supportati i tipi di contenuto Cyclical e Ordered ma l'algoritmo li considera come valori discreti e non esegue un'elaborazione speciale.</span><span class="sxs-lookup"><span data-stu-id="87a5d-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="87a5d-275">Vedere anche</span><span class="sxs-lookup"><span data-stu-id="87a5d-275">See Also</span></span>  
 <span data-ttu-id="87a5d-276">[Algoritmo Microsoft Neural Network](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="87a5d-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="87a5d-277">[Contenuto del modello di data mining per i modelli di rete neurale &#40;Analysis Services-Data mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="87a5d-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="87a5d-278">Esempi di query sul modello di rete neurale</span><span class="sxs-lookup"><span data-stu-id="87a5d-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
