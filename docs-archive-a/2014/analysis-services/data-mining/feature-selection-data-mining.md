---
title: Selezione delle caratteristiche (data mining) | Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 08/04/2020
ms.locfileid: "87630159"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="e61fa-102">Selezione delle caratteristiche (Data mining)</span><span class="sxs-lookup"><span data-stu-id="e61fa-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="e61fa-103">La *selezione delle funzioni* è un termine comunemente usato in data mining per descrivere gli strumenti e le tecniche disponibili per ridurre gli input a una dimensione gestibile per l'elaborazione e l'analisi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="e61fa-104">La selezione delle funzioni implica non solo la *riduzione della cardinalità*, ovvero l'imposizione di un limite arbitrario o predefinito per il numero di attributi che possono essere considerati durante la compilazione di un modello, ma anche la scelta degli attributi, vale a dire che l'analista o lo strumento di modellazione seleziona o Elimina gli attributi in base all'utilità per l'analisi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="e61fa-105">La possibilità di applicare la selezione delle caratteristiche è essenziale per un'analisi efficace, poiché nei set di dati sono spesso contenute molte più informazioni rispetto a quelle necessarie per compilare il modello.</span><span class="sxs-lookup"><span data-stu-id="e61fa-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="e61fa-106">Ad esempio, in un set di dati potrebbero essere contenute 500 colonne in cui vengono descritte le caratteristiche dei clienti; tuttavia, se i dati di alcune colonne sono di tipo sparse, non sarebbe molto vantaggioso aggiungerli al modello.</span><span class="sxs-lookup"><span data-stu-id="e61fa-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="e61fa-107">Se si mantengono le colonne non necessarie durante la compilazione del modello, la quantità di CPU e memoria necessaria per il processo di training sarà maggiore, così come lo spazio di archiviazione richiesto per il modello completato.</span><span class="sxs-lookup"><span data-stu-id="e61fa-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="e61fa-108">Anche se le risorse non costituiscono un problema, è generalmente consigliabile rimuovere le colonne non necessarie che potrebbero avere un impatto negativo sulla qualità dei modelli individuati per i motivi seguenti:</span><span class="sxs-lookup"><span data-stu-id="e61fa-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="e61fa-109">Alcune colonne sono poco significative o ridondanti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="e61fa-110">In questo caso risulta più difficile individuare modelli significativi dai dati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="e61fa-111">Per individuare modelli di qualità, la maggior parte degli algoritmi di data mining richiede un set di dati di training di dimensioni molto maggiori in set di dati di dimensioni elevate,</span><span class="sxs-lookup"><span data-stu-id="e61fa-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="e61fa-112">ma i dati di training hanno dimensioni ridotte in alcune applicazioni di data mining.</span><span class="sxs-lookup"><span data-stu-id="e61fa-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="e61fa-113">Se solo in 50 delle 500 colonne dell'origine dati sono presenti informazioni utili per la compilazione di un modello, è possibile semplicemente non includerle nel modello oppure utilizzare le tecniche di selezione delle caratteristiche per individuare automaticamente le caratteristiche migliori ed escludere i valori che sono statisticamente insignificanti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="e61fa-114">La selezione delle caratteristiche consente di risolvere i problemi relativi alla presenza di una quantità eccessiva di dati di scarso valore o di una quantità ridotta di dati di valore elevato.</span><span class="sxs-lookup"><span data-stu-id="e61fa-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="e61fa-115">Selezione delle caratteristiche nel data mining in Analysis Services</span><span class="sxs-lookup"><span data-stu-id="e61fa-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="e61fa-116">Generalmente, la selezione delle caratteristiche viene eseguita automaticamente in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] e in ogni algoritmo è disponibile un set di tecniche predefinite per applicare in modo intelligente la riduzione delle caratteristiche.</span><span class="sxs-lookup"><span data-stu-id="e61fa-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="e61fa-117">La selezione delle caratteristiche viene sempre eseguita prima del training del modello per scegliere automaticamente gli attributi di un set di dati che hanno maggiori probabilità di essere utilizzati nel modello.</span><span class="sxs-lookup"><span data-stu-id="e61fa-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="e61fa-118">Tuttavia, è anche possibile impostare manualmente i parametri per influire sul comportamento della selezione delle caratteristiche.</span><span class="sxs-lookup"><span data-stu-id="e61fa-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="e61fa-119">In genere, la selezione delle caratteristiche consente di calcolare un punteggio per ogni attributo, quindi di selezionare solo gli attributi con i punteggi migliori.</span><span class="sxs-lookup"><span data-stu-id="e61fa-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="e61fa-120">Inoltre, è possibile regolare la soglia per i punteggi massimi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-120">You can also adjust the threshold for the top scores.</span></span> <span data-ttu-id="e61fa-121">In [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] sono disponibili più metodi per calcolare questi punteggi e il metodo esatto applicato in qualsiasi modello dipende dai fattori seguenti:</span><span class="sxs-lookup"><span data-stu-id="e61fa-121">[!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="e61fa-122">Algoritmo utilizzato nel modello</span><span class="sxs-lookup"><span data-stu-id="e61fa-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="e61fa-123">Tipo di dati dell'attributo</span><span class="sxs-lookup"><span data-stu-id="e61fa-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="e61fa-124">Qualsiasi parametro che potrebbe essere stato impostato sul modello</span><span class="sxs-lookup"><span data-stu-id="e61fa-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="e61fa-125">La selezione delle caratteristiche viene applicata agli input, agli attributi stimabili oppure agli stati di una colonna.</span><span class="sxs-lookup"><span data-stu-id="e61fa-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="e61fa-126">Quando il calcolo del punteggio per la selezione delle caratteristiche è terminato, nel processo di compilazione del modello vengono inclusi solo gli attributi e gli stati selezionati dall'algoritmo, che possono essere utilizzati per eventuali stime.</span><span class="sxs-lookup"><span data-stu-id="e61fa-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="e61fa-127">Se si sceglie un attributo stimabile che non soddisfa la soglia per la selezione delle caratteristiche, l'attributo può comunque essere utilizzato per le stime, tuttavia queste ultime saranno basate unicamente sulle statistiche globali presenti nel modello.</span><span class="sxs-lookup"><span data-stu-id="e61fa-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="e61fa-128">La selezione delle caratteristiche influisce solo sulle colonne utilizzate nel modello, mentre non ha effetto sull'archiviazione della struttura di data mining.</span><span class="sxs-lookup"><span data-stu-id="e61fa-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="e61fa-129">Le colonne rimosse dal modello di data mining sono ancora disponibili nella struttura e i dati delle colonne della struttura di data mining verranno memorizzati nella cache.</span><span class="sxs-lookup"><span data-stu-id="e61fa-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="e61fa-130">Definizione di metodi per implementare la caratteristica di selezione degli attributi</span><span class="sxs-lookup"><span data-stu-id="e61fa-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="e61fa-131">È possibile implementare la caratteristica di selezione degli attributi in molti modi, a seconda del tipo di dati utilizzati e dell'algoritmo scelto per l'analisi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="e61fa-132">In SQL Server Analysis Services sono disponibili diversi metodi diffusi e consolidati per il punteggio degli attributi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="e61fa-133">Il metodo applicato in qualsiasi algoritmo o set di dati dipende dai tipi di dati e dall'utilizzo delle colonne.</span><span class="sxs-lookup"><span data-stu-id="e61fa-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="e61fa-134">Il punteggio di *interesse* viene usato per classificare e ordinare gli attributi in colonne che contengono dati numerici continui non binari.</span><span class="sxs-lookup"><span data-stu-id="e61fa-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="e61fa-135">L'*entropia di Shannon* e due punteggi *Bayes* sono disponibili per le colonne contenenti dati discreti e discretizzati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="e61fa-136">Tuttavia, se nel modello sono contenute colonne continue, il punteggio di interesse verrà utilizzato per valutare tutte le colonne di input e assicurare la coerenza.</span><span class="sxs-lookup"><span data-stu-id="e61fa-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="e61fa-137">Nella sezione seguente viene descritto ogni metodo relativo alla selezione delle caratteristiche.</span><span class="sxs-lookup"><span data-stu-id="e61fa-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="e61fa-138">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-138">Interestingness score</span></span>  
 <span data-ttu-id="e61fa-139">Una caratteristica è interessante se indica informazioni utili.</span><span class="sxs-lookup"><span data-stu-id="e61fa-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="e61fa-140">Poiché la definizione di ciò che risulta utile varia a seconda dello scenario, il data mining settore ha sviluppato diversi modi per misurare l' *interesse*.</span><span class="sxs-lookup"><span data-stu-id="e61fa-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="e61fa-141">Ad esempio, la *novità* potrebbe essere interessante nel rilevamento degli outlier, ma la possibilità di discriminare tra elementi strettamente correlati, o il *peso discriminante*, potrebbe essere più interessante per la classificazione.</span><span class="sxs-lookup"><span data-stu-id="e61fa-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="e61fa-142">La misura dell'interesse utilizzato nel SQL Server Analysis Services è *basata sull'entropia*, ovvero gli attributi con distribuzioni casuali hanno un'entropia più elevata e un miglioramento delle informazioni inferiore; tali attributi sono pertanto meno interessanti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="e61fa-143">L'entropia di un determinato attributo viene confrontata con quella di tutti gli altri attributi, come segue:</span><span class="sxs-lookup"><span data-stu-id="e61fa-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="e61fa-144">Interesse(Attributo) = - (m - Entropia(Attributo)) \* (m - Entropia(Attributo))</span><span class="sxs-lookup"><span data-stu-id="e61fa-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="e61fa-145">Per entropia centrale o m si intende l'entropia dell'intero set di caratteristiche.</span><span class="sxs-lookup"><span data-stu-id="e61fa-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="e61fa-146">Sottraendo l'entropia dell'attributo di destinazione dall'entropia centrale, è possibile valutare la quantità di informazioni fornita dall'attributo.</span><span class="sxs-lookup"><span data-stu-id="e61fa-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="e61fa-147">Questo punteggio viene utilizzato per impostazione predefinita ogni volta che la colonna contiene dati numerici continui non binari.</span><span class="sxs-lookup"><span data-stu-id="e61fa-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="e61fa-148">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="e61fa-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="e61fa-149">L'entropia di Shannon misura l'incertezza di una variabile casuale per un determinato risultato.</span><span class="sxs-lookup"><span data-stu-id="e61fa-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="e61fa-150">Ad esempio, l'entropia del lancio di una moneta può essere rappresentata come funzione della probabilità che esca testa.</span><span class="sxs-lookup"><span data-stu-id="e61fa-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="e61fa-151">In Analysis Services viene utilizzata la formula seguente per il calcolo dell'entropia di Shannon:</span><span class="sxs-lookup"><span data-stu-id="e61fa-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="e61fa-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="e61fa-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="e61fa-153">Questo metodo di assegnazione dei punteggi è disponibile per gli attributi discreti e discretizzati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="e61fa-154">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="e61fa-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="e61fa-155">In Analysis Services sono disponibili due punteggi della caratteristica di selezione degli attributi basati su reti Bayes.</span><span class="sxs-lookup"><span data-stu-id="e61fa-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="e61fa-156">Una rete Bayes è un grafico *orientato* o *aciclico* di stati e di transizioni tra stati, ossia alcuni stati sono sempre precedenti allo stato corrente, alcuni sono successivi e il grafico non si ripete.</span><span class="sxs-lookup"><span data-stu-id="e61fa-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="e61fa-157">Per definizione, le reti Bayes consentono l'utilizzo delle conoscenze precedenti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="e61fa-158">Tuttavia, la domanda in merito a quali stati precedenti utilizzare nel calcolo delle probabilità degli stati successivi è importante per la progettazione, le prestazioni e l'accuratezza dell'algoritmo.</span><span class="sxs-lookup"><span data-stu-id="e61fa-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="e61fa-159">L'algoritmo K2 per l'apprendimento di una rete Bayes, sviluppato da Cooper e Herskovits, viene utilizzato spesso nel data mining.</span><span class="sxs-lookup"><span data-stu-id="e61fa-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="e61fa-160">È scalabile e consente di analizzare più variabili, ma richiede l'ordinamento delle variabili utilizzate come input.</span><span class="sxs-lookup"><span data-stu-id="e61fa-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="e61fa-161">Per altre informazioni, vedere [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) di Chickering, Geiger e Heckerman.</span><span class="sxs-lookup"><span data-stu-id="e61fa-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="e61fa-162">Questo metodo di assegnazione dei punteggi è disponibile per gli attributi discreti e discretizzati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="e61fa-163">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme</span><span class="sxs-lookup"><span data-stu-id="e61fa-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="e61fa-164">Anche per il punteggio equivalente Bayes Dirichlet (BDE, Bayesian Dirichlet Equivalent) si utilizza l'analisi bayesiana per valutare una rete dato un set di dati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="e61fa-165">Il metodo di assegnazione dei punteggi BDE, sviluppato da Heckerman, è basato sulla metrica BD sviluppata da Cooper e Herskovits.</span><span class="sxs-lookup"><span data-stu-id="e61fa-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="e61fa-166">La distribuzione Dirichlet è una distribuzione multinomiale che descrive la probabilità condizionale di ogni variabile nella rete, oltre a includere molte proprietà utili per l'apprendimento.</span><span class="sxs-lookup"><span data-stu-id="e61fa-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="e61fa-167">Il metodo equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (BDEU, Bayesian Dirichlet Equivalent with Uniform Prior) presuppone un caso speciale della distribuzione Dirichlet in cui viene utilizzata una costante matematica per creare una distribuzione fissa o uniforme di stati precedenti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="e61fa-168">Il punteggio BDE presuppone anche un'equivalenza di probabilità, ossia non è possibile prevedere che i dati discriminino strutture equivalenti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="e61fa-169">In altri termini, se il punteggio di Se A allora B è uguale al punteggio di Se B allora A, non è possibile distinguere le strutture in base ai dati e non è possibile derivare il rapporto di causa ed effetto.</span><span class="sxs-lookup"><span data-stu-id="e61fa-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="e61fa-170">Per altre informazioni sulle reti Bayes e sull'implementazione di questi metodi di assegnazione dei punteggi, vedere [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span><span class="sxs-lookup"><span data-stu-id="e61fa-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="e61fa-171">Metodi per implementare la caratteristica di selezione degli attributi utilizzati dagli algoritmi di Analysis Services</span><span class="sxs-lookup"><span data-stu-id="e61fa-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="e61fa-172">Nella tabella seguente sono elencati gli algoritmi che supportano la caratteristica di selezione degli attributi, i metodi utilizzati dall'algoritmo e i parametri impostati per controllare il funzionamento di questa caratteristica:</span><span class="sxs-lookup"><span data-stu-id="e61fa-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="e61fa-173">Algoritmo</span><span class="sxs-lookup"><span data-stu-id="e61fa-173">Algorithm</span></span>|<span data-ttu-id="e61fa-174">Metodo di analisi</span><span class="sxs-lookup"><span data-stu-id="e61fa-174">Method of analysis</span></span>|<span data-ttu-id="e61fa-175">Commenti</span><span class="sxs-lookup"><span data-stu-id="e61fa-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="e61fa-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="e61fa-176">Naive Bayes</span></span>|<span data-ttu-id="e61fa-177">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="e61fa-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="e61fa-178">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="e61fa-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="e61fa-179">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="e61fa-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="e61fa-180">Poiché l'algoritmo Microsoft Naive Bayes accetta solo attributi discreti o discretizzati, non può utilizzare il punteggio di interesse.</span><span class="sxs-lookup"><span data-stu-id="e61fa-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="e61fa-181">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Naive Bayes](microsoft-naive-bayes-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-182">Alberi delle decisioni</span><span class="sxs-lookup"><span data-stu-id="e61fa-182">Decision trees</span></span>|<span data-ttu-id="e61fa-183">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="e61fa-184">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="e61fa-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="e61fa-185">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="e61fa-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="e61fa-186">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="e61fa-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="e61fa-187">Se esistono colonne contenenti valori continui non binari, viene utilizzato il punteggio di interesse per tutte le colonne, per assicurare coerenza.</span><span class="sxs-lookup"><span data-stu-id="e61fa-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="e61fa-188">In caso contrario, viene utilizzato il metodo per implementare la caratteristica di selezione degli attributi predefinito oppure il metodo specificato quando è stato creato il modello.</span><span class="sxs-lookup"><span data-stu-id="e61fa-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="e61fa-189">Per altre informazioni su questo algoritmo, vedere [Guida di riferimento tecnico per l'algoritmo Microsoft Decision Trees](microsoft-decision-trees-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-190">Neural Network</span><span class="sxs-lookup"><span data-stu-id="e61fa-190">Neural network</span></span>|<span data-ttu-id="e61fa-191">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="e61fa-192">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="e61fa-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="e61fa-193">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="e61fa-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="e61fa-194">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="e61fa-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="e61fa-195">Nell'algoritmo Microsoft Neural Network possono essere utilizzati sia i metodi Bayes sia quelli basati sull'entropia, purché nei dati siano contenute colonne continue.</span><span class="sxs-lookup"><span data-stu-id="e61fa-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="e61fa-196">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Neural Network](microsoft-neural-network-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-197">Logistic Regression</span><span class="sxs-lookup"><span data-stu-id="e61fa-197">Logistic regression</span></span>|<span data-ttu-id="e61fa-198">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="e61fa-199">Entropia di Shannon</span><span class="sxs-lookup"><span data-stu-id="e61fa-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="e61fa-200">Bayes con probabilità a priori K2</span><span class="sxs-lookup"><span data-stu-id="e61fa-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="e61fa-201">Equivalente Bayes Dirichlet con probabilità a priori a distribuzione uniforme (impostazione predefinita)</span><span class="sxs-lookup"><span data-stu-id="e61fa-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="e61fa-202">Sebbene l'algoritmo Microsoft Logistic Regression sia basato sull'algoritmo Microsoft Neural Network, non è possibile personalizzare modelli di regressione logistica per controllare il comportamento della caratteristica di selezione degli attributi. Di conseguenza tale caratteristica viene impostata automaticamente sempre sul metodo più appropriato per l'attributo.</span><span class="sxs-lookup"><span data-stu-id="e61fa-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="e61fa-203">Se tutti gli attributi sono discreti o discretizzati, l'impostazione predefinita è BDEU.</span><span class="sxs-lookup"><span data-stu-id="e61fa-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="e61fa-204">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Logistic Regression](microsoft-logistic-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-205">Clustering</span><span class="sxs-lookup"><span data-stu-id="e61fa-205">Clustering</span></span>|<span data-ttu-id="e61fa-206">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-206">Interestingness score</span></span>|<span data-ttu-id="e61fa-207">L'algoritmo Microsoft Clustering può utilizzare dati discreti o discretizzati.</span><span class="sxs-lookup"><span data-stu-id="e61fa-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="e61fa-208">Tuttavia, perché il punteggio di ogni attributo è calcolato come una distanza e viene rappresentato come un numero continuo, è necessario utilizzare il punteggio di interesse.</span><span class="sxs-lookup"><span data-stu-id="e61fa-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="e61fa-209">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Clustering](microsoft-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-210">Linear Regression</span><span class="sxs-lookup"><span data-stu-id="e61fa-210">Linear regression</span></span>|<span data-ttu-id="e61fa-211">Punteggio di interesse</span><span class="sxs-lookup"><span data-stu-id="e61fa-211">Interestingness score</span></span>|<span data-ttu-id="e61fa-212">L'algoritmo Microsoft Linear Regression può utilizzare solo il punteggio di interesse, poiché supporta solo colonne continue.</span><span class="sxs-lookup"><span data-stu-id="e61fa-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="e61fa-213">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Linear Regression](microsoft-linear-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-214">Regole di associazione</span><span class="sxs-lookup"><span data-stu-id="e61fa-214">Association rules</span></span><br /><br /> <span data-ttu-id="e61fa-215">Sequence Clustering</span><span class="sxs-lookup"><span data-stu-id="e61fa-215">Sequence clustering</span></span>|<span data-ttu-id="e61fa-216">Non usato</span><span class="sxs-lookup"><span data-stu-id="e61fa-216">Not used</span></span>|<span data-ttu-id="e61fa-217">La caratteristica di selezione degli attributi non viene richiamata da questi algoritmi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="e61fa-218">È tuttavia possibile controllare il comportamento dell'algoritmo e ridurre le dimensioni dei dati di input impostando i valori dei parametri MINIMUM_SUPPORT e MINIMUM_PROBABILIITY.</span><span class="sxs-lookup"><span data-stu-id="e61fa-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="e61fa-219">Per altre informazioni, vedere [Riferimento tecnico per l'algoritmo Microsoft Association Rules](microsoft-association-algorithm-technical-reference.md) e [Riferimento tecnico per l'algoritmo Microsoft Sequence Clustering](microsoft-sequence-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="e61fa-220">Serie temporale</span><span class="sxs-lookup"><span data-stu-id="e61fa-220">Time series</span></span>|<span data-ttu-id="e61fa-221">Non usato</span><span class="sxs-lookup"><span data-stu-id="e61fa-221">Not used</span></span>|<span data-ttu-id="e61fa-222">La selezione delle caratteristiche non si applica ai modelli Time Series.</span><span class="sxs-lookup"><span data-stu-id="e61fa-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="e61fa-223">Per altre informazioni su questo algoritmo, vedere [Riferimento tecnico per l'algoritmo Microsoft Time Series](microsoft-time-series-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="e61fa-224">Parametri della selezione delle caratteristiche</span><span class="sxs-lookup"><span data-stu-id="e61fa-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="e61fa-225">Negli algoritmi che supportano la selezione delle caratteristiche è possibile controllare quando tale selezione è abilitata tramite i parametri riportati di seguito.</span><span class="sxs-lookup"><span data-stu-id="e61fa-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="e61fa-226">In ogni algoritmo è disponibile un valore predefinito per il numero di input consentiti; tuttavia è possibile eseguire l'override di questo valore predefinito e specificare il numero di attributi.</span><span class="sxs-lookup"><span data-stu-id="e61fa-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="e61fa-227">In questa sezione sono elencati i parametri forniti per la gestione della selezione delle caratteristiche.</span><span class="sxs-lookup"><span data-stu-id="e61fa-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="e61fa-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="e61fa-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="e61fa-229">Se in un modello è contenuto un numero di colonne maggiore del numero specificato nel parametro *MAXIMUM_INPUT_ATTRIBUTES* , vengono ignorate le colonne che in base all'algoritmo non risultano di interesse.</span><span class="sxs-lookup"><span data-stu-id="e61fa-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="e61fa-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="e61fa-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="e61fa-231">In modo analogo, se un modello contiene un numero di colonne stimabili maggiore del numero specificato nel parametro *MAXIMUM_OUTPUT_ATTRIBUTES* , vengono ignorate le colonne che in base all'algoritmo non risultano di interesse.</span><span class="sxs-lookup"><span data-stu-id="e61fa-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="e61fa-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="e61fa-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="e61fa-233">Se un modello contiene un numero di case maggiore del numero specificato nel parametro *MAXIMUM_STATES* , gli stati usati meno di frequente vengono raggruppati e considerati mancanti.</span><span class="sxs-lookup"><span data-stu-id="e61fa-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="e61fa-234">Se uno di questi parametri è impostato su 0, la selezione delle caratteristiche viene disabilitata con relativo impatto sui tempi di elaborazione e sulle prestazioni.</span><span class="sxs-lookup"><span data-stu-id="e61fa-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="e61fa-235">Oltre a questi metodi per la selezione delle caratteristiche, è possibile migliorare la capacità dell'algoritmo di identificare o promuovere attributi significativi impostando i *flag di modellazione* sul modello oppure i *flag di distribuzione* sulla struttura.</span><span class="sxs-lookup"><span data-stu-id="e61fa-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="e61fa-236">Per altre informazioni su questi concetti, vedere [Flag di modellazione &#40;Data mining&#41;](modeling-flags-data-mining.md) e [Distribuzioni delle colonne &#40;Data mining&#41;](column-distributions-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="e61fa-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="e61fa-237">Vedere anche</span><span class="sxs-lookup"><span data-stu-id="e61fa-237">See Also</span></span>  
 [<span data-ttu-id="e61fa-238">Personalizzare struttura e modelli di data mining</span><span class="sxs-lookup"><span data-stu-id="e61fa-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
